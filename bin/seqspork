#!/bin/bash

# Getting started
#
# Compile spark with hadoop 2.2.0 -SPARK_HADOOP_VERSION=2.2.0 SPARK_YARN=true ./sbt/sbt clean assembly to generate spark.jar

# Compile pig with -Dhadoopversion=23 flag

# Configure following environment variables to run it on YARN cluster


# Follow this guide for for enabling running spork:
##  http://docs.sigmoidanalytics.com/index.php/Setting_up_spork_with_spark_0.8.1

if [ -z "$HADOOP_CONF_DIR" ]; then
    echo "You need to set HADOOP_CONF_DIR"
    exit 1
fi
if [ -z "$HADOOP_HOME" ]; then
    echo "You need to set HADOOP_HOME"
    exit 1
fi

# Settings to work with YARN, spark jar compiled with hadoop 2
export SPARK_MASTER=yarn-client

# Simple wrapper script to start pig with with SeqPig-related jars and
# set the udf.import.list property to fi.aalto.seqpig.

if [ -z "${SEQPIG_HOME}" ]; then
	SEQPIG_HOME="`dirname $(readlink -f $0)`/../"
fi

source "${SEQPIG_HOME}/bin/seqpigEnv.sh"

import_list=fi.aalto.seqpig:fi.aalto.seqpig.io:fi.aalto.seqpig.filter:fi.aalto.seqpig.pileup:fi.aalto.seqpig.stats:myudf

echo "pig.additional.jars: ${SEQPIG_JARS}"
echo "udf.import.list: ${import_list}"

export PIG_CLASSPATH=$PIG_HOME/build/ivy/lib/Pig:$PIG_HOME/pig-0.15.0-SNAPSHOT-core-h2.jar:$HADOOP_CONF_DIR:$PIG_HOME/conf:$SPARK_HOME/assembly/target/scala-2.10/spark-assembly-1.0.0-hadoop2.2.0.jar:$SPARK_HOME/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.6.0.jar
export PIG_CLASSPATH=$PIG_CLASSPATH::${SEQPIG_JARS}
export SPARK_PIG_JAR=$PIG_HOME/pig-0.15.0-SNAPSHOT-core-h2.jar

# Cluster settings
export SPARK_EXECUTOR_CORES=4
export SPARK_EXECUTOR_MEMORY=1200m
export SPARK_MEM=1200m
export SPARK_EXECUTOR_INSTANCES=2
export SPARK_PARALLELISM=2;
export SPARK_REPARTITION_LOAD=true
export SPARK_STORAGE_LEVEL="MEMORY_AND_DISK"
#echo "${PIG_HOME}/bin/pig -x spark  ${@}"
${PIG_HOME}/bin/pig -x spark "$@"







